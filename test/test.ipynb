{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afd1799",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "\n",
    "\n",
    "Polynomial = jnp.ndarray\n",
    "\n",
    "\n",
    "def poly_x() -> Polynomial:\n",
    "    return jnp.array([1, 0])\n",
    "\n",
    "\n",
    "def poly_int(coeffs: Polynomial) -> Polynomial:\n",
    "    return jnp.concatenate([coeffs / jnp.arange(len(coeffs), 0, -1), jnp.zeros(1)])\n",
    "\n",
    "\n",
    "def poly_definite_int(coeffs: Polynomial, l: float, r: float) -> float:\n",
    "    integral = poly_int(coeffs)\n",
    "    return jnp.polyval(integral, r) - jnp.polyval(integral, l)\n",
    "\n",
    "\n",
    "def poly_shift(p: Polynomial, h: float) -> Polynomial:\n",
    "    \"\"\"\n",
    "    p(x) -> p(x - h)\n",
    "    \"\"\"\n",
    "\n",
    "    res = jnp.zeros([1])\n",
    "\n",
    "    x_m_h = jnp.array([1, -h])\n",
    "    x_m_h_p = jnp.ones([1])\n",
    "\n",
    "    for i in range(len(p)):\n",
    "        res = jnp.polyadd(res, x_m_h_p * p[-i - 1])\n",
    "        x_m_h_p = jnp.polymul(x_m_h_p, x_m_h)\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7090a405",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这个文件定义了两个“张量列车/张量链”相关的数据结构：\n",
    "\n",
    "1) TT: Tensor Train（张量列车）——用一串三维 core 来表示一个高维张量\n",
    "   - 每个 core 的形状是 (r_left, dim, r_right)\n",
    "   - r_left, r_right 是“TT rank”（内部连接的秩）\n",
    "   - dim 是这个维度的物理维度（例如每个变量离散取值个数）\n",
    "\n",
    "2) TTOperator: TT 格式的线性算子（矩阵/算子）——用一串四维 core 来表示一个高维线性变换\n",
    "   - 每个 core 的形状是 (r_left, dim_from, dim_to, r_right)\n",
    "   - dim_from 是输入维度，dim_to 是输出维度\n",
    "\n",
    "此外提供一些常用操作：\n",
    "- 生成全 0 的 TT\n",
    "- 随机生成 TT / TTOperator\n",
    "- 把 TT / Operator “还原成完整张量/完整算子”（full_tensor / full_operator）\n",
    "- TT 的 reverse / astype / 减法\n",
    "- TT core 的转置（用于 reverse）\n",
    "- 两个 TT 的减法（返回一个新的 TT，秩会增大）\n",
    "\n",
    "注意：代码用 flax.struct.dataclass 让这个类是“不可变 pytree”，方便 JAX 的 jit/vmap/grad。\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Sequence, List\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from flax import struct\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TT:\n",
    "    \"\"\"\n",
    "    Tensor Train (TT) 表示法。\n",
    "\n",
    "    一个 n 维张量 A[i1, i2, ..., in] 被表示成 n 个 core 的连乘：\n",
    "      core_k 的形状是 (r_{k}, dim_k, r_{k+1})\n",
    "    其中：\n",
    "      - dim_k 是第 k 维的大小（物理维度）\n",
    "      - r_k 是 TT rank（内部连接维度）\n",
    "      - 约定 r_0 = r_{n} = 1，这样整条链最终收缩成标量/张量元素\n",
    "\n",
    "    这里 cores 存放的是一个 list，每个元素是 jnp.ndarray（三维）\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, dims: Sequence[int], rs: Sequence[int]) -> TT:\n",
    "        \"\"\"\n",
    "        构造一个全 0 的 TT。\n",
    "\n",
    "        参数：\n",
    "          dims: 每个维度的大小 [dim1, dim2, ..., dim_n]\n",
    "          rs:   TT ranks（不包含两端的 1）[r1, r2, ..., r_{n-1}]\n",
    "                注意长度必须是 n-1\n",
    "\n",
    "        返回：\n",
    "          TT 对象，其中每个 core 都是全 0 数组。\n",
    "        \"\"\"\n",
    "        # TT 的标准约束：n 个 dims 对应 n-1 个内部 rank\n",
    "        assert len(dims) == len(rs) + 1\n",
    "\n",
    "        # 两端 rank 固定为 1：r0=1, rn=1\n",
    "        rs = [1] + list(rs) + [1]\n",
    "\n",
    "        # 逐个维度创建 core：形状 (r_left, dim, r_right)\n",
    "        cores = [jnp.zeros((rs[i], dim, rs[i + 1])) for i, dim in enumerate(dims)]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    @classmethod\n",
    "    def generate_random(cls, key: jnp.ndarray, dims: Sequence[int], rs: Sequence[int]) -> TT:\n",
    "        \"\"\"\n",
    "        随机生成一个 TT（每个 core 元素 ~ N(0,1)）。\n",
    "\n",
    "        参数：\n",
    "          key:  JAX 随机数 key\n",
    "          dims: 每个维度大小\n",
    "          rs:   内部 ranks（长度 n-1）\n",
    "\n",
    "        返回：\n",
    "          TT 对象，cores 为随机正态。\n",
    "        \"\"\"\n",
    "        assert len(dims) == len(rs) + 1\n",
    "\n",
    "        rs = [1] + list(rs) + [1]\n",
    "\n",
    "        # 为每个 core 分配一个子 key，避免随机数重复\n",
    "        keys = jax.random.split(key, len(dims))\n",
    "\n",
    "        # 对每个维度 dim 生成 (r_left, dim, r_right) 的随机 core\n",
    "        cores = [\n",
    "            jax.random.normal(key, (rs[i], dim, rs[i + 1]))\n",
    "            for i, (dim, key) in enumerate(zip(dims, keys))\n",
    "        ]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    # TT 的核心数据：n 个 core，每个 core 是 (r_left, dim, r_right)\n",
    "    cores: List[jnp.ndarray]\n",
    "\n",
    "    @property\n",
    "    def n_dims(self):\n",
    "        \"\"\"返回张量的维数 n（也就是 core 的个数）。\"\"\"\n",
    "        return len(self.cores)\n",
    "\n",
    "    @property\n",
    "    def full_tensor(self) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        把 TT 还原成“完整的高维张量”。\n",
    "\n",
    "        实现方式：\n",
    "          从第一个 core 开始，依次与后续 core 做 einsum 收缩 TT rank 维度。\n",
    "          每次收缩掉上一个结果的右 rank，与下一个 core 的左 rank 对齐。\n",
    "\n",
    "        结果形状：\n",
    "          (dim1, dim2, ..., dim_n)\n",
    "        \"\"\"\n",
    "        res = self.cores[0]  # (1, dim1, r2)\n",
    "        for core in self.cores[1:]:\n",
    "            # res:  (..., r)   core: (r, i, R)\n",
    "            # -> (..., i, R)  把 r 收缩掉，拼接出新的物理维度 i\n",
    "            res = jnp.einsum('...r,riR->...iR', res, core)\n",
    "\n",
    "        # TT 两端 rank 都是 1，所以第一维和最后一维可以 squeeze 掉\n",
    "        return jnp.squeeze(res, (0, -1))\n",
    "\n",
    "    def reverse(self) -> TT:\n",
    "        \"\"\"\n",
    "        把 TT 的维度顺序反过来（核心顺序翻转）。\n",
    "\n",
    "        注意：\n",
    "          翻转 core 的顺序后，每个 core 的左右 rank 方向也反了，\n",
    "          所以需要对 core 做 transpose_core 来交换左右 rank 轴。\n",
    "        \"\"\"\n",
    "        return TT([transpose_core(core) for core in self.cores[::-1]])\n",
    "\n",
    "    def astype(self, dtype: jnp.dtype) -> TT:\n",
    "        \"\"\"把 TT 的所有 core 转成指定 dtype（例如 float32 / float64）。\"\"\"\n",
    "        return TT([core.astype(dtype) for core in self.cores])\n",
    "\n",
    "    def __sub__(self, other: TT):\n",
    "        \"\"\"定义 TT 的减法：self - other。\"\"\"\n",
    "        return subtract(self, other)\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTOperator:\n",
    "    \"\"\"\n",
    "    TT 格式的线性算子（你可以理解成高维矩阵/算子）。\n",
    "\n",
    "    如果普通矩阵是 2D：A[out, in]\n",
    "    那高维算子可以看成：A[i1..in, j1..jn]（输入 n 维 -> 输出 n 维）\n",
    "\n",
    "    TT Operator 用 n 个 4D core 表示，每个 core 形状：\n",
    "      (r_left, dim_from, dim_to, r_right)\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def generate_random(\n",
    "        cls, key: jnp.ndarray, dims_from: Sequence[int], dims_to: Sequence[int], rs: Sequence[int]\n",
    "    ) -> TTOperator:\n",
    "        \"\"\"\n",
    "        随机生成一个 TT Operator（每个 core 元素 ~ N(0,1)）。\n",
    "\n",
    "        参数：\n",
    "          key:       JAX random key\n",
    "          dims_from: 输入每维大小 [din1, din2, ..., din_n]\n",
    "          dims_to:   输出每维大小 [dout1, dout2, ..., dout_n]\n",
    "          rs:        内部 ranks（长度 n-1）\n",
    "\n",
    "        返回：\n",
    "          TTOperator 对象\n",
    "        \"\"\"\n",
    "        n_dims = len(dims_from)\n",
    "\n",
    "        # 基本一致性检查\n",
    "        assert len(dims_from) == n_dims\n",
    "        assert len(dims_to) == n_dims\n",
    "        assert len(rs) + 1 == n_dims\n",
    "\n",
    "        rs = [1] + list(rs) + [1]\n",
    "        keys = jax.random.split(key, n_dims)\n",
    "\n",
    "        # 每个 core 是 4D：(r_left, dim_from, dim_to, r_right)\n",
    "        cores = [\n",
    "            jax.random.normal(key, (rs[i], dim_from, dim_to, rs[i + 1]))\n",
    "            for i, (dim_from, dim_to, key) in enumerate(zip(dims_from, dims_to, keys))\n",
    "        ]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    cores: List[jnp.ndarray]\n",
    "\n",
    "    @property\n",
    "    def full_operator(self) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        把 TT Operator 还原成完整算子（一个巨大的高维张量/矩阵）。\n",
    "\n",
    "        逐 core einsum：\n",
    "          res:  (..., r)\n",
    "          core: (r, i, j, R)\n",
    "          -> (..., i, j, R)\n",
    "\n",
    "        最终 squeeze 掉两端 rank=1。\n",
    "        结果形状：\n",
    "          (din1, dout1, din2, dout2, ..., din_n, dout_n)\n",
    "        （具体排列取决于 einsum 的写法，这里是按每个维度生成一对 (i,j)）\n",
    "        \"\"\"\n",
    "        res = self.cores[0]\n",
    "        for core in self.cores[1:]:\n",
    "            res = jnp.einsum('...r,rijR->...ijR', res, core) # Einstein求和约定\n",
    "        return jnp.squeeze(res, (0, -1)) \n",
    "\n",
    "    def reverse(self):\n",
    "        \"\"\"\n",
    "        反转 operator 的 core 顺序。\n",
    "\n",
    "        这里作者留了句注释：\n",
    "          \"idk, what should I do with axes 1 and 2.\"\n",
    "        意思是：输入/输出物理轴 (dim_from, dim_to) 是否要交换、怎么交换，\n",
    "        其实取决于你希望 reverse 后代表什么数学对象（是反转维度顺序？还是转置算子？）。\n",
    "\n",
    "        当前实现：\n",
    "          - core 顺序翻转\n",
    "          - 把 rank 轴对调：把 axis 0 和 axis 3 互换\n",
    "          - axis 1/2（输入/输出物理轴）保持不变\n",
    "        \"\"\"\n",
    "        return TTOperator(\n",
    "            [jnp.moveaxis(core, (0, 1, 2, 3), (3, 1, 2, 0)) for core in self.cores[::-1]]\n",
    "        )\n",
    "\n",
    "\n",
    "def transpose_core(core: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    对 TT core 做“左右 rank 交换”。\n",
    "\n",
    "    输入 core 形状：(r_left, dim, r_right)\n",
    "    输出形状：(r_right, dim, r_left)\n",
    "\n",
    "    用途：\n",
    "      TT.reverse() 翻转 core 顺序时，需要把连接方向也翻过来。\n",
    "    \"\"\"\n",
    "    return jnp.moveaxis(core, (0, 1, 2), (2, 1, 0))\n",
    "\n",
    "\n",
    "def subtract(lhs: TT, rhs: TT) -> TT:\n",
    "    \"\"\"\n",
    "    计算两个 TT 的差：lhs - rhs，并返回一个新的 TT。\n",
    "\n",
    "    关键点（非常重要）：\n",
    "    - 一般 TT 直接相减后，结果的 TT rank 会变大\n",
    "    - 这里用的是经典的“block-diagonal 拼接”构造法：\n",
    "        - 第一个 core：在右 rank 方向拼接 [lhs, -rhs]\n",
    "        - 中间 core：做 2x2 的块对角拼接（lhs 在左上，rhs 在右下）\n",
    "        - 最后一个 core：在左 rank 方向拼接 [lhs; rhs]\n",
    "\n",
    "    这样保证：\n",
    "      TT(full) = TT(lhs) - TT(rhs)\n",
    "\n",
    "    代价：\n",
    "      ranks 会变成原来的大约“相加”（更准确：中间 rank 变成 r1+r2）。\n",
    "    \"\"\"\n",
    "    assert lhs.n_dims == rhs.n_dims\n",
    "\n",
    "    # 只有 1 个维度时，TT 就是一个 (1, dim, 1) 的 core，直接相减即可\n",
    "    if lhs.n_dims == 1:\n",
    "        return TT([lhs.cores[0] - rhs.cores[0]])\n",
    "\n",
    "    # 第一个 core：沿着右 rank 维（axis=-1）拼接\n",
    "    # lhs: (1, d1, r1)  rhs: (1, d1, r1')\n",
    "    # -> (1, d1, r1+r1')\n",
    "    first = jnp.concatenate([lhs.cores[0], -rhs.cores[0]], axis=-1)\n",
    "\n",
    "    # 最后一个 core：沿着左 rank 维（axis=0）拼接\n",
    "    # lhs: (r_{n-1}, dn, 1)  rhs: (r'_{n-1}, dn, 1)\n",
    "    # -> (r_{n-1}+r'_{n-1}, dn, 1)\n",
    "    last = jnp.concatenate([lhs.cores[-1], rhs.cores[-1]], axis=0)\n",
    "\n",
    "    # 中间 core：做块对角拼接（2x2 block）\n",
    "    # 对每个位置 k：\n",
    "    #   [ c1   0 ]\n",
    "    #   [ 0   c2 ]\n",
    "    inner = [\n",
    "        jnp.concatenate(\n",
    "            [\n",
    "                # 上半块： [c1, 0]\n",
    "                jnp.concatenate([c1, jnp.zeros((c1.shape[0], c1.shape[1], c2.shape[2]))], axis=-1),\n",
    "                # 下半块： [0, c2]\n",
    "                jnp.concatenate([jnp.zeros((c2.shape[0], c2.shape[1], c1.shape[2])), c2], axis=-1),\n",
    "            ],\n",
    "            axis=0,  # 沿左 rank 方向拼接成上下两块\n",
    "        )\n",
    "        for c1, c2 in zip(lhs.cores[1:-1], rhs.cores[1:-1])\n",
    "    ]\n",
    "\n",
    "    return TT([first] + inner + [last])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565b772d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2026-02-08 13:09:48,953:jax._src.xla_bridge:876: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "dims = [4, 3, 5]   # 三个物理维度\n",
    "rs   = [2, 3]      # 内部 ranks: r1=2, r2=3（长度 n-1）\n",
    "\n",
    "tt1 = TT.generate_random(key, dims=dims, rs=rs)\n",
    "tt2 = TT.generate_random(key, dims=dims, rs=rs)\n",
    "tt3 = tt1-tt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679b11cd",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4.7683716e-07, dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = tt1.full_tensor\n",
    "B1 = tt2.full_tensor\n",
    "C1 = tt3.full_tensor\n",
    "C1[1,1,1] - A1[1,1,1] + B1[1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac2bc8a",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 5]\n",
      "[4, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "dims = [G.shape[1] for G in tt1.cores]\n",
    "print(dims)\n",
    "\n",
    "dims = [G.shape[1] for G in tt3.cores]\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2abe766",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 2)\n",
      "(1, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tt1.cores[0].shape)\n",
    "print(tt3.cores[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96e6d65",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import struct\n",
    "from jax import numpy as jnp, ops, vmap\n",
    "\n",
    "from ttde.tt.tensors import TT, TTOperator\n",
    "from ttde.utils import cached_einsum\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTOpt:\n",
    "    first: jnp.ndarray\n",
    "    inner: jnp.ndarray\n",
    "    last: jnp.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, n_dims: int, dim: int, rank: int):\n",
    "        return TTOpt(jnp.zeros([1, dim, rank]), jnp.zeros([n_dims - 2, rank, dim, rank]), jnp.zeros([rank, dim, 1]))\n",
    "\n",
    "    @classmethod\n",
    "    def from_tt(cls, tt: TT):\n",
    "        return cls(tt.cores[0], jnp.stack(tt.cores[1:-1], axis=0), tt.cores[-1])\n",
    "\n",
    "    @classmethod\n",
    "    def rank_1_from_vectors(cls, vectors: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        vectors: [N_DIMS, DIM]\n",
    "        \"\"\"\n",
    "        return cls(vectors[0, None, :, None], vectors[1:-1, None, :, None], vectors[-1, None, :, None])\n",
    "\n",
    "    @classmethod\n",
    "    def from_canonical(cls, vectors: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        vectors: [RANK, N_DIMS, DIM]\n",
    "        \"\"\"\n",
    "        first = vectors[:, 0, :, None].T\n",
    "\n",
    "        inner = jnp.zeros([vectors.shape[1] - 2, vectors.shape[0], vectors.shape[2], vectors.shape[0]])\n",
    "        inner = inner.at[:, jnp.arange(vectors.shape[0]), :, jnp.arange(vectors.shape[0])].set(vectors[:, 1:-1, :])\n",
    "\n",
    "        last = vectors[:, -1, :, None]\n",
    "\n",
    "        return cls(first, inner, last)\n",
    "\n",
    "    @property\n",
    "    def n_dims(self) -> int:\n",
    "        return 2 + self.inner.shape[0]\n",
    "\n",
    "    def to_nonopt_tt(self):\n",
    "        return TT([self.first, *self.inner, self.last])\n",
    "\n",
    "    def abs(self) -> 'TTOpt':\n",
    "        return TTOpt(jnp.abs(self.first), jnp.abs(self.inner), jnp.abs(self.last))\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTOperatorOpt:\n",
    "    first: jnp.ndarray\n",
    "    inner: jnp.ndarray\n",
    "    last: jnp.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def from_tt_operator(cls, tt: TTOperator):\n",
    "        return cls(tt.cores[0], jnp.stack(tt.cores[1:-1], axis=0), tt.cores[-1])\n",
    "\n",
    "    @classmethod\n",
    "    def rank_1_from_matrices(cls, matrices: jnp.ndarray):\n",
    "        return cls(matrices[0, None, :, :, None], matrices[1:-1, None, :, :, None], matrices[-1, None, :, :, None])\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizedValue:\n",
    "    value: jnp.ndarray\n",
    "    log_norm: float\n",
    "\n",
    "    @classmethod\n",
    "    def from_value(cls, value):\n",
    "        sqr_norm = (value ** 2).sum()\n",
    "        norm_is_zero = sqr_norm == 0\n",
    "        updated_sqr_norm = jnp.where(norm_is_zero, 1., sqr_norm)\n",
    "\n",
    "        return cls(\n",
    "            log_norm=jnp.where(norm_is_zero, -jnp.inf, .5 * jnp.log(updated_sqr_norm)),\n",
    "            value=value / jnp.sqrt(updated_sqr_norm)\n",
    "        )\n",
    "\n",
    "\n",
    "def normalized_inner_product(tt1: TTOpt, tt2: TTOpt):\n",
    "    def body(state, cores):\n",
    "        G1, G2 = cores\n",
    "        contracted = NormalizedValue.from_value(cached_einsum('ij,ikl,jkn->ln', state.value, G1, G2))\n",
    "        return (\n",
    "            NormalizedValue(\n",
    "                value=contracted.value,\n",
    "                log_norm=jnp.where(state.log_norm == -jnp.inf, -jnp.inf, state.log_norm + contracted.log_norm)\n",
    "            ),\n",
    "            None\n",
    "        )\n",
    "\n",
    "    state = NormalizedValue.from_value(cached_einsum('ikl,jkn->ln', tt1.first, tt2.first))\n",
    "    state, _ = jax.lax.scan(body, state, (tt1.inner, tt2.inner))\n",
    "    state, _ = body(state, (tt1.last, tt2.last))\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def normalized_dot_operator(tt: TTOpt, tt_op: TTOperatorOpt):\n",
    "    def body(x, A):\n",
    "        c = jnp.einsum('rms,tmnu->rtnsu', x, A)\n",
    "        return c.reshape(c.shape[0] * c.shape[1], c.shape[2], c.shape[3] * c.shape[4])\n",
    "\n",
    "    return TTOpt(\n",
    "        body(tt.first, tt_op.first),\n",
    "        vmap(body)(tt.inner, tt_op.inner),\n",
    "        body(tt.last, tt_op.last)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f25958",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (2529352709.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m*A.inner\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "A = TTOpt.zeros(4,4,4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
