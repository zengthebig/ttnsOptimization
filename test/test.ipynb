{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afd1799",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "\n",
    "\n",
    "Polynomial = jnp.ndarray\n",
    "\n",
    "\n",
    "def poly_x() -> Polynomial:\n",
    "    return jnp.array([1, 0])\n",
    "\n",
    "\n",
    "def poly_int(coeffs: Polynomial) -> Polynomial:\n",
    "    return jnp.concatenate([coeffs / jnp.arange(len(coeffs), 0, -1), jnp.zeros(1)])\n",
    "\n",
    "\n",
    "def poly_definite_int(coeffs: Polynomial, l: float, r: float) -> float:\n",
    "    integral = poly_int(coeffs)\n",
    "    return jnp.polyval(integral, r) - jnp.polyval(integral, l)\n",
    "\n",
    "\n",
    "def poly_shift(p: Polynomial, h: float) -> Polynomial:\n",
    "    \"\"\"\n",
    "    p(x) -> p(x - h)\n",
    "    \"\"\"\n",
    "\n",
    "    res = jnp.zeros([1])\n",
    "\n",
    "    x_m_h = jnp.array([1, -h])\n",
    "    x_m_h_p = jnp.ones([1])\n",
    "\n",
    "    for i in range(len(p)):\n",
    "        res = jnp.polyadd(res, x_m_h_p * p[-i - 1])\n",
    "        x_m_h_p = jnp.polymul(x_m_h_p, x_m_h)\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7090a405",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这个文件定义了两个“张量列车/张量链”相关的数据结构：\n",
    "\n",
    "1) TT: Tensor Train（张量列车）——用一串三维 core 来表示一个高维张量\n",
    "   - 每个 core 的形状是 (r_left, dim, r_right)\n",
    "   - r_left, r_right 是“TT rank”（内部连接的秩）\n",
    "   - dim 是这个维度的物理维度（例如每个变量离散取值个数）\n",
    "\n",
    "2) TTOperator: TT 格式的线性算子（矩阵/算子）——用一串四维 core 来表示一个高维线性变换\n",
    "   - 每个 core 的形状是 (r_left, dim_from, dim_to, r_right)\n",
    "   - dim_from 是输入维度，dim_to 是输出维度\n",
    "\n",
    "此外提供一些常用操作：\n",
    "- 生成全 0 的 TT\n",
    "- 随机生成 TT / TTOperator\n",
    "- 把 TT / Operator “还原成完整张量/完整算子”（full_tensor / full_operator）\n",
    "- TT 的 reverse / astype / 减法\n",
    "- TT core 的转置（用于 reverse）\n",
    "- 两个 TT 的减法（返回一个新的 TT，秩会增大）\n",
    "\n",
    "注意：代码用 flax.struct.dataclass 让这个类是“不可变 pytree”，方便 JAX 的 jit/vmap/grad。\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Sequence, List\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from flax import struct\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TT:\n",
    "    \"\"\"\n",
    "    Tensor Train (TT) 表示法。\n",
    "\n",
    "    一个 n 维张量 A[i1, i2, ..., in] 被表示成 n 个 core 的连乘：\n",
    "      core_k 的形状是 (r_{k}, dim_k, r_{k+1})\n",
    "    其中：\n",
    "      - dim_k 是第 k 维的大小（物理维度）\n",
    "      - r_k 是 TT rank（内部连接维度）\n",
    "      - 约定 r_0 = r_{n} = 1，这样整条链最终收缩成标量/张量元素\n",
    "\n",
    "    这里 cores 存放的是一个 list，每个元素是 jnp.ndarray（三维）\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, dims: Sequence[int], rs: Sequence[int]) -> TT:\n",
    "        \"\"\"\n",
    "        构造一个全 0 的 TT。\n",
    "\n",
    "        参数：\n",
    "          dims: 每个维度的大小 [dim1, dim2, ..., dim_n]\n",
    "          rs:   TT ranks（不包含两端的 1）[r1, r2, ..., r_{n-1}]\n",
    "                注意长度必须是 n-1\n",
    "\n",
    "        返回：\n",
    "          TT 对象，其中每个 core 都是全 0 数组。\n",
    "        \"\"\"\n",
    "        # TT 的标准约束：n 个 dims 对应 n-1 个内部 rank\n",
    "        assert len(dims) == len(rs) + 1\n",
    "\n",
    "        # 两端 rank 固定为 1：r0=1, rn=1\n",
    "        rs = [1] + list(rs) + [1]\n",
    "\n",
    "        # 逐个维度创建 core：形状 (r_left, dim, r_right)\n",
    "        cores = [jnp.zeros((rs[i], dim, rs[i + 1])) for i, dim in enumerate(dims)]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    @classmethod\n",
    "    def generate_random(cls, key: jnp.ndarray, dims: Sequence[int], rs: Sequence[int]) -> TT:\n",
    "        \"\"\"\n",
    "        随机生成一个 TT（每个 core 元素 ~ N(0,1)）。\n",
    "\n",
    "        参数：\n",
    "          key:  JAX 随机数 key\n",
    "          dims: 每个维度大小\n",
    "          rs:   内部 ranks（长度 n-1）\n",
    "\n",
    "        返回：\n",
    "          TT 对象，cores 为随机正态。\n",
    "        \"\"\"\n",
    "        assert len(dims) == len(rs) + 1\n",
    "\n",
    "        rs = [1] + list(rs) + [1]\n",
    "\n",
    "        # 为每个 core 分配一个子 key，避免随机数重复\n",
    "        keys = jax.random.split(key, len(dims))\n",
    "\n",
    "        # 对每个维度 dim 生成 (r_left, dim, r_right) 的随机 core\n",
    "        cores = [\n",
    "            jax.random.normal(key, (rs[i], dim, rs[i + 1]))\n",
    "            for i, (dim, key) in enumerate(zip(dims, keys))\n",
    "        ]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    # TT 的核心数据：n 个 core，每个 core 是 (r_left, dim, r_right)\n",
    "    cores: List[jnp.ndarray]\n",
    "\n",
    "    @property\n",
    "    def n_dims(self):\n",
    "        \"\"\"返回张量的维数 n（也就是 core 的个数）。\"\"\"\n",
    "        return len(self.cores)\n",
    "\n",
    "    @property\n",
    "    def full_tensor(self) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        把 TT 还原成“完整的高维张量”。\n",
    "\n",
    "        实现方式：\n",
    "          从第一个 core 开始，依次与后续 core 做 einsum 收缩 TT rank 维度。\n",
    "          每次收缩掉上一个结果的右 rank，与下一个 core 的左 rank 对齐。\n",
    "\n",
    "        结果形状：\n",
    "          (dim1, dim2, ..., dim_n)\n",
    "        \"\"\"\n",
    "        res = self.cores[0]  # (1, dim1, r2)\n",
    "        for core in self.cores[1:]:\n",
    "            # res:  (..., r)   core: (r, i, R)\n",
    "            # -> (..., i, R)  把 r 收缩掉，拼接出新的物理维度 i\n",
    "            res = jnp.einsum('...r,riR->...iR', res, core)\n",
    "\n",
    "        # TT 两端 rank 都是 1，所以第一维和最后一维可以 squeeze 掉\n",
    "        return jnp.squeeze(res, (0, -1))\n",
    "\n",
    "    def reverse(self) -> TT:\n",
    "        \"\"\"\n",
    "        把 TT 的维度顺序反过来（核心顺序翻转）。\n",
    "\n",
    "        注意：\n",
    "          翻转 core 的顺序后，每个 core 的左右 rank 方向也反了，\n",
    "          所以需要对 core 做 transpose_core 来交换左右 rank 轴。\n",
    "        \"\"\"\n",
    "        return TT([transpose_core(core) for core in self.cores[::-1]])\n",
    "\n",
    "    def astype(self, dtype: jnp.dtype) -> TT:\n",
    "        \"\"\"把 TT 的所有 core 转成指定 dtype（例如 float32 / float64）。\"\"\"\n",
    "        return TT([core.astype(dtype) for core in self.cores])\n",
    "\n",
    "    def __sub__(self, other: TT):\n",
    "        \"\"\"定义 TT 的减法：self - other。\"\"\"\n",
    "        return subtract(self, other)\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTOperator:\n",
    "    \"\"\"\n",
    "    TT 格式的线性算子（你可以理解成高维矩阵/算子）。\n",
    "\n",
    "    如果普通矩阵是 2D：A[out, in]\n",
    "    那高维算子可以看成：A[i1..in, j1..jn]（输入 n 维 -> 输出 n 维）\n",
    "\n",
    "    TT Operator 用 n 个 4D core 表示，每个 core 形状：\n",
    "      (r_left, dim_from, dim_to, r_right)\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def generate_random(\n",
    "        cls, key: jnp.ndarray, dims_from: Sequence[int], dims_to: Sequence[int], rs: Sequence[int]\n",
    "    ) -> TTOperator:\n",
    "        \"\"\"\n",
    "        随机生成一个 TT Operator（每个 core 元素 ~ N(0,1)）。\n",
    "\n",
    "        参数：\n",
    "          key:       JAX random key\n",
    "          dims_from: 输入每维大小 [din1, din2, ..., din_n]\n",
    "          dims_to:   输出每维大小 [dout1, dout2, ..., dout_n]\n",
    "          rs:        内部 ranks（长度 n-1）\n",
    "\n",
    "        返回：\n",
    "          TTOperator 对象\n",
    "        \"\"\"\n",
    "        n_dims = len(dims_from)\n",
    "\n",
    "        # 基本一致性检查\n",
    "        assert len(dims_from) == n_dims\n",
    "        assert len(dims_to) == n_dims\n",
    "        assert len(rs) + 1 == n_dims\n",
    "\n",
    "        rs = [1] + list(rs) + [1]\n",
    "        keys = jax.random.split(key, n_dims)\n",
    "\n",
    "        # 每个 core 是 4D：(r_left, dim_from, dim_to, r_right)\n",
    "        cores = [\n",
    "            jax.random.normal(key, (rs[i], dim_from, dim_to, rs[i + 1]))\n",
    "            for i, (dim_from, dim_to, key) in enumerate(zip(dims_from, dims_to, keys))\n",
    "        ]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    cores: List[jnp.ndarray]\n",
    "\n",
    "    @property\n",
    "    def full_operator(self) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        把 TT Operator 还原成完整算子（一个巨大的高维张量/矩阵）。\n",
    "\n",
    "        逐 core einsum：\n",
    "          res:  (..., r)\n",
    "          core: (r, i, j, R)\n",
    "          -> (..., i, j, R)\n",
    "\n",
    "        最终 squeeze 掉两端 rank=1。\n",
    "        结果形状：\n",
    "          (din1, dout1, din2, dout2, ..., din_n, dout_n)\n",
    "        （具体排列取决于 einsum 的写法，这里是按每个维度生成一对 (i,j)）\n",
    "        \"\"\"\n",
    "        res = self.cores[0]\n",
    "        for core in self.cores[1:]:\n",
    "            res = jnp.einsum('...r,rijR->...ijR', res, core) # Einstein求和约定\n",
    "        return jnp.squeeze(res, (0, -1)) \n",
    "\n",
    "    def reverse(self):\n",
    "        \"\"\"\n",
    "        反转 operator 的 core 顺序。\n",
    "\n",
    "        这里作者留了句注释：\n",
    "          \"idk, what should I do with axes 1 and 2.\"\n",
    "        意思是：输入/输出物理轴 (dim_from, dim_to) 是否要交换、怎么交换，\n",
    "        其实取决于你希望 reverse 后代表什么数学对象（是反转维度顺序？还是转置算子？）。\n",
    "\n",
    "        当前实现：\n",
    "          - core 顺序翻转\n",
    "          - 把 rank 轴对调：把 axis 0 和 axis 3 互换\n",
    "          - axis 1/2（输入/输出物理轴）保持不变\n",
    "        \"\"\"\n",
    "        return TTOperator(\n",
    "            [jnp.moveaxis(core, (0, 1, 2, 3), (3, 1, 2, 0)) for core in self.cores[::-1]]\n",
    "        )\n",
    "\n",
    "\n",
    "def transpose_core(core: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    对 TT core 做“左右 rank 交换”。\n",
    "\n",
    "    输入 core 形状：(r_left, dim, r_right)\n",
    "    输出形状：(r_right, dim, r_left)\n",
    "\n",
    "    用途：\n",
    "      TT.reverse() 翻转 core 顺序时，需要把连接方向也翻过来。\n",
    "    \"\"\"\n",
    "    return jnp.moveaxis(core, (0, 1, 2), (2, 1, 0))\n",
    "\n",
    "\n",
    "def subtract(lhs: TT, rhs: TT) -> TT:\n",
    "    \"\"\"\n",
    "    计算两个 TT 的差：lhs - rhs，并返回一个新的 TT。\n",
    "\n",
    "    关键点（非常重要）：\n",
    "    - 一般 TT 直接相减后，结果的 TT rank 会变大\n",
    "    - 这里用的是经典的“block-diagonal 拼接”构造法：\n",
    "        - 第一个 core：在右 rank 方向拼接 [lhs, -rhs]\n",
    "        - 中间 core：做 2x2 的块对角拼接（lhs 在左上，rhs 在右下）\n",
    "        - 最后一个 core：在左 rank 方向拼接 [lhs; rhs]\n",
    "\n",
    "    这样保证：\n",
    "      TT(full) = TT(lhs) - TT(rhs)\n",
    "\n",
    "    代价：\n",
    "      ranks 会变成原来的大约“相加”（更准确：中间 rank 变成 r1+r2）。\n",
    "    \"\"\"\n",
    "    assert lhs.n_dims == rhs.n_dims\n",
    "\n",
    "    # 只有 1 个维度时，TT 就是一个 (1, dim, 1) 的 core，直接相减即可\n",
    "    if lhs.n_dims == 1:\n",
    "        return TT([lhs.cores[0] - rhs.cores[0]])\n",
    "\n",
    "    # 第一个 core：沿着右 rank 维（axis=-1）拼接\n",
    "    # lhs: (1, d1, r1)  rhs: (1, d1, r1')\n",
    "    # -> (1, d1, r1+r1')\n",
    "    first = jnp.concatenate([lhs.cores[0], -rhs.cores[0]], axis=-1)\n",
    "\n",
    "    # 最后一个 core：沿着左 rank 维（axis=0）拼接\n",
    "    # lhs: (r_{n-1}, dn, 1)  rhs: (r'_{n-1}, dn, 1)\n",
    "    # -> (r_{n-1}+r'_{n-1}, dn, 1)\n",
    "    last = jnp.concatenate([lhs.cores[-1], rhs.cores[-1]], axis=0)\n",
    "\n",
    "    # 中间 core：做块对角拼接（2x2 block）\n",
    "    # 对每个位置 k：\n",
    "    #   [ c1   0 ]\n",
    "    #   [ 0   c2 ]\n",
    "    inner = [\n",
    "        jnp.concatenate(\n",
    "            [\n",
    "                # 上半块： [c1, 0]\n",
    "                jnp.concatenate([c1, jnp.zeros((c1.shape[0], c1.shape[1], c2.shape[2]))], axis=-1),\n",
    "                # 下半块： [0, c2]\n",
    "                jnp.concatenate([jnp.zeros((c2.shape[0], c2.shape[1], c1.shape[2])), c2], axis=-1),\n",
    "            ],\n",
    "            axis=0,  # 沿左 rank 方向拼接成上下两块\n",
    "        )\n",
    "        for c1, c2 in zip(lhs.cores[1:-1], rhs.cores[1:-1])\n",
    "    ]\n",
    "\n",
    "    return TT([first] + inner + [last])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565b772d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "dims = [4, 3, 5]   # 三个物理维度\n",
    "rs   = [2, 3]      # 内部 ranks: r1=2, r2=3（长度 n-1）\n",
    "\n",
    "tt1 = TT.generate_random(key, dims=dims, rs=rs)\n",
    "tt2 = TT.generate_random(key, dims=dims, rs=rs)\n",
    "tt3 = tt1-tt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "679b11cd",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4.7683716e-07, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = tt1.full_tensor\n",
    "B1 = tt2.full_tensor\n",
    "C1 = tt3.full_tensor\n",
    "C1[1,1,1] - A1[1,1,1] + B1[1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cac2bc8a",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 5]\n",
      "[4, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "dims = [G.shape[1] for G in tt1.cores]\n",
    "print(dims)\n",
    "\n",
    "dims = [G.shape[1] for G in tt3.cores]\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2abe766",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 2)\n",
      "(1, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tt1.cores[0].shape)\n",
    "print(tt3.cores[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96e6d65",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import struct\n",
    "from jax import numpy as jnp, ops, vmap\n",
    "\n",
    "from ttde.tt.tensors import TT, TTOperator\n",
    "from ttde.utils import cached_einsum\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTOpt:\n",
    "    first: jnp.ndarray\n",
    "    inner: jnp.ndarray\n",
    "    last: jnp.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, n_dims: int, dim: int, rank: int):\n",
    "        return TTOpt(jnp.zeros([1, dim, rank]), jnp.zeros([n_dims - 2, rank, dim, rank]), jnp.zeros([rank, dim, 1]))\n",
    "\n",
    "    @classmethod\n",
    "    def from_tt(cls, tt: TT):\n",
    "        return cls(tt.cores[0], jnp.stack(tt.cores[1:-1], axis=0), tt.cores[-1])\n",
    "\n",
    "    @classmethod\n",
    "    def rank_1_from_vectors(cls, vectors: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        vectors: [N_DIMS, DIM]\n",
    "        \"\"\"\n",
    "        return cls(vectors[0, None, :, None], vectors[1:-1, None, :, None], vectors[-1, None, :, None])\n",
    "\n",
    "    @classmethod\n",
    "    def from_canonical(cls, vectors: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        vectors: [RANK, N_DIMS, DIM]\n",
    "        \"\"\"\n",
    "        first = vectors[:, 0, :, None].T\n",
    "\n",
    "        inner = jnp.zeros([vectors.shape[1] - 2, vectors.shape[0], vectors.shape[2], vectors.shape[0]])\n",
    "        inner = inner.at[:, jnp.arange(vectors.shape[0]), :, jnp.arange(vectors.shape[0])].set(vectors[:, 1:-1, :])\n",
    "\n",
    "        last = vectors[:, -1, :, None]\n",
    "\n",
    "        return cls(first, inner, last)\n",
    "\n",
    "    @property\n",
    "    def n_dims(self) -> int:\n",
    "        return 2 + self.inner.shape[0]\n",
    "\n",
    "    def to_nonopt_tt(self):\n",
    "        return TT([self.first, *self.inner, self.last])\n",
    "\n",
    "    def abs(self) -> 'TTOpt':\n",
    "        return TTOpt(jnp.abs(self.first), jnp.abs(self.inner), jnp.abs(self.last))\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTOperatorOpt:\n",
    "    first: jnp.ndarray\n",
    "    inner: jnp.ndarray\n",
    "    last: jnp.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def from_tt_operator(cls, tt: TTOperator):\n",
    "        return cls(tt.cores[0], jnp.stack(tt.cores[1:-1], axis=0), tt.cores[-1])\n",
    "\n",
    "    @classmethod\n",
    "    def rank_1_from_matrices(cls, matrices: jnp.ndarray):\n",
    "        return cls(matrices[0, None, :, :, None], matrices[1:-1, None, :, :, None], matrices[-1, None, :, :, None])\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizedValue:\n",
    "    value: jnp.ndarray\n",
    "    log_norm: float\n",
    "\n",
    "    @classmethod\n",
    "    def from_value(cls, value):\n",
    "        sqr_norm = (value ** 2).sum()\n",
    "        norm_is_zero = sqr_norm == 0\n",
    "        updated_sqr_norm = jnp.where(norm_is_zero, 1., sqr_norm)\n",
    "\n",
    "        return cls(\n",
    "            log_norm=jnp.where(norm_is_zero, -jnp.inf, .5 * jnp.log(updated_sqr_norm)),\n",
    "            value=value / jnp.sqrt(updated_sqr_norm)\n",
    "        )\n",
    "\n",
    "\n",
    "def normalized_inner_product(tt1: TTOpt, tt2: TTOpt):\n",
    "    def body(state, cores):\n",
    "        G1, G2 = cores\n",
    "        contracted = NormalizedValue.from_value(cached_einsum('ij,ikl,jkn->ln', state.value, G1, G2))\n",
    "        return (\n",
    "            NormalizedValue(\n",
    "                value=contracted.value,\n",
    "                log_norm=jnp.where(state.log_norm == -jnp.inf, -jnp.inf, state.log_norm + contracted.log_norm)\n",
    "            ),\n",
    "            None\n",
    "        )\n",
    "\n",
    "    state = NormalizedValue.from_value(cached_einsum('ikl,jkn->ln', tt1.first, tt2.first))\n",
    "    state, _ = jax.lax.scan(body, state, (tt1.inner, tt2.inner))\n",
    "    state, _ = body(state, (tt1.last, tt2.last))\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def normalized_dot_operator(tt: TTOpt, tt_op: TTOperatorOpt):\n",
    "    def body(x, A):\n",
    "        c = jnp.einsum('rms,tmnu->rtnsu', x, A)\n",
    "        return c.reshape(c.shape[0] * c.shape[1], c.shape[2], c.shape[3] * c.shape[4])\n",
    "\n",
    "    return TTOpt(\n",
    "        body(tt.first, tt_op.first),\n",
    "        vmap(body)(tt.inner, tt_op.inner),\n",
    "        body(tt.last, tt_op.last)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f25958",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这个文件定义了两个“张量列车/张量链”相关的数据结构：\n",
    "\n",
    "1) TT: Tensor Train（张量列车）——用一串三维 core 来表示一个高维张量\n",
    "   - 每个 core 的形状是 (r_left, dim, r_right)\n",
    "   - r_left, r_right 是“TT rank”（内部连接的秩）\n",
    "   - dim 是这个维度的物理维度（例如每个变量离散取值个数）\n",
    "\n",
    "2) TTOperator: TT 格式的线性算子（矩阵/算子）——用一串四维 core 来表示一个高维线性变换\n",
    "   - 每个 core 的形状是 (r_left, dim_from, dim_to, r_right)\n",
    "   - dim_from 是输入维度，dim_to 是输出维度\n",
    "\n",
    "此外提供一些常用操作：\n",
    "- 生成全 0 的 TT\n",
    "- 随机生成 TT / TTOperator\n",
    "- 把 TT / Operator “还原成完整张量/完整算子”（full_tensor / full_operator）\n",
    "- TT 的 reverse / astype / 减法\n",
    "- TT core 的转置（用于 reverse）\n",
    "- 两个 TT 的减法（返回一个新的 TT，秩会增大）\n",
    "\n",
    "注意：代码用 flax.struct.dataclass 让这个类是“不可变 pytree”，方便 JAX 的 jit/vmap/grad。\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Sequence, List, Optional\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from flax import struct\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTNS:\n",
    "    \"\"\"Tensor Train Network State (tree-structured).\n",
    "\n",
    "    Dimension convention for each core ``G_k``:\n",
    "        G_k[alpha_parent, i_k, alpha_child1, alpha_child2, ...]\n",
    "\n",
    "    ``alpha_parent`` is the virtual dimension to the parent (size 1 for root),\n",
    "    ``i_k`` is the physical dimension, and the remaining axes correspond to the\n",
    "    children in the order given by ``neighbors[k]`` with the parent filtered out.\n",
    "    \"\"\"\n",
    "\n",
    "    cores: List[jnp.ndarray]\n",
    "    neighbors: List[List[int]]\n",
    "    root: Optional[int] = None\n",
    "    parent: Optional[List[int]] = None\n",
    "\n",
    "    @property\n",
    "    def n_nodes(self) -> int:\n",
    "        return len(self.cores)\n",
    "\n",
    "    @property\n",
    "    def n_dims(self) -> int:\n",
    "        return self.n_nodes\n",
    "\n",
    "    def _build_parent(self, root: int) -> List[int]:\n",
    "        parent = [-1] * self.n_nodes\n",
    "        parent[root] = root\n",
    "        stack = [root]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            for nbr in self.neighbors[node]:\n",
    "                if parent[nbr] != -1:\n",
    "                    continue\n",
    "                parent[nbr] = node\n",
    "                stack.append(nbr)\n",
    "        return parent\n",
    "\n",
    "    def _resolve_parent(self) -> List[int]:\n",
    "        if self.parent is not None:\n",
    "            assert len(self.parent) == self.n_nodes\n",
    "            return list(self.parent)\n",
    "        root = 0 if self.root is None else self.root\n",
    "        return self._build_parent(root)\n",
    "\n",
    "    def _resolve_root(self, parent: List[int]) -> int:\n",
    "        if self.root is not None:\n",
    "            return self.root\n",
    "        if -1 in parent:\n",
    "            return parent.index(-1)\n",
    "        for idx, value in enumerate(parent):\n",
    "            if idx == value:\n",
    "                return idx\n",
    "        return 0\n",
    "\n",
    "    def validate_tree(self) -> None:\n",
    "        n_nodes = self.n_nodes\n",
    "        assert len(self.neighbors) == n_nodes\n",
    "\n",
    "        for node, nbrs in enumerate(self.neighbors):\n",
    "            assert node not in nbrs\n",
    "            assert len(set(nbrs)) == len(nbrs)\n",
    "            for nbr in nbrs:\n",
    "                assert 0 <= nbr < n_nodes\n",
    "                assert node in self.neighbors[nbr]\n",
    "\n",
    "        parent = self._resolve_parent()\n",
    "        root = self._resolve_root(parent)\n",
    "        assert 0 <= root < n_nodes\n",
    "\n",
    "        visited = set()\n",
    "        stack = [(root, -1)]\n",
    "        while stack:\n",
    "            node, prev = stack.pop()\n",
    "            if node in visited:\n",
    "                raise AssertionError(\"Graph contains a cycle\")\n",
    "            visited.add(node)\n",
    "            for nbr in self.neighbors[node]:\n",
    "                if nbr == prev:\n",
    "                    continue\n",
    "                stack.append((nbr, node))\n",
    "        assert len(visited) == n_nodes\n",
    "\n",
    "        for node, core in enumerate(self.cores):\n",
    "            degree = len(self.neighbors[node])\n",
    "            assert core.ndim == 2 + degree\n",
    "\n",
    "        children_by_node = []\n",
    "        for node in range(n_nodes):\n",
    "            node_parent = parent[node]\n",
    "            children = [nbr for nbr in self.neighbors[node] if nbr != node_parent]\n",
    "            children_by_node.append(children)\n",
    "\n",
    "        for node, core in enumerate(self.cores):\n",
    "            if node == root:\n",
    "                assert core.shape[0] == 1\n",
    "                continue\n",
    "            parent_node = parent[node]\n",
    "            parent_children = children_by_node[parent_node]\n",
    "            child_index = parent_children.index(node)\n",
    "            parent_axis = 2 + child_index\n",
    "            assert core.shape[0] == self.cores[parent_node].shape[parent_axis]\n",
    "\n",
    "    def postorder(self) -> List[int]:\n",
    "        parent = self._resolve_parent()\n",
    "        root = self._resolve_root(parent)\n",
    "        children_by_node = []\n",
    "        for node in range(self.n_nodes):\n",
    "            node_parent = parent[node]\n",
    "            children = [nbr for nbr in self.neighbors[node] if nbr != node_parent]\n",
    "            children_by_node.append(children)\n",
    "\n",
    "        order = []\n",
    "        stack = [(root, 0)]\n",
    "        while stack:\n",
    "            node, idx = stack.pop()\n",
    "            children = children_by_node[node]\n",
    "            if idx < len(children):\n",
    "                stack.append((node, idx + 1))\n",
    "                stack.append((children[idx], 0))\n",
    "            else:\n",
    "                order.append(node)\n",
    "        return order\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TT:\n",
    "    \"\"\"\n",
    "    Tensor Train (TT) 表示法。\n",
    "\n",
    "    一个 n 维张量 A[i1, i2, ..., in] 被表示成 n 个 core 的连乘：\n",
    "      core_k 的形状是 (r_{k}, dim_k, r_{k+1})\n",
    "    其中：\n",
    "      - dim_k 是第 k 维的大小（物理维度）\n",
    "      - r_k 是 TT rank（内部连接维度）\n",
    "      - 约定 r_0 = r_{n} = 1，这样整条链最终收缩成标量/张量元素\n",
    "\n",
    "    这里 cores 存放的是一个 list，每个元素是 jnp.ndarray（三维）\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, dims: Sequence[int], rs: Sequence[int]) -> TT:\n",
    "        \"\"\"\n",
    "        构造一个全 0 的 TT。\n",
    "\n",
    "        参数：\n",
    "          dims: 每个维度的大小 [dim1, dim2, ..., dim_n]\n",
    "          rs:   TT ranks（不包含两端的 1）[r1, r2, ..., r_{n-1}]\n",
    "                注意长度必须是 n-1\n",
    "\n",
    "        返回：\n",
    "          TT 对象，其中每个 core 都是全 0 数组。\n",
    "        \"\"\"\n",
    "        # TT 的标准约束：n 个 dims 对应 n-1 个内部 rank\n",
    "        assert len(dims) == len(rs) + 1\n",
    "\n",
    "        # 两端 rank 固定为 1：r0=1, rn=1\n",
    "        rs = [1] + list(rs) + [1]\n",
    "\n",
    "        # 逐个维度创建 core：形状 (r_left, dim, r_right)\n",
    "        cores = [jnp.zeros((rs[i], dim, rs[i + 1])) for i, dim in enumerate(dims)]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    @classmethod\n",
    "    def generate_random(cls, key: jnp.ndarray, dims: Sequence[int], rs: Sequence[int]) -> TT:\n",
    "        \"\"\"\n",
    "        随机生成一个 TT（每个 core 元素 ~ N(0,1)）。\n",
    "\n",
    "        参数：\n",
    "          key:  JAX 随机数 key\n",
    "          dims: 每个维度大小\n",
    "          rs:   内部 ranks（长度 n-1）\n",
    "\n",
    "        返回：\n",
    "          TT 对象，cores 为随机正态。\n",
    "        \"\"\"\n",
    "        assert len(dims) == len(rs) + 1\n",
    "\n",
    "        rs = [1] + list(rs) + [1]\n",
    "\n",
    "        # 为每个 core 分配一个子 key，避免随机数重复\n",
    "        keys = jax.random.split(key, len(dims))\n",
    "\n",
    "        # 对每个维度 dim 生成 (r_left, dim, r_right) 的随机 core\n",
    "        cores = [\n",
    "            jax.random.normal(key, (rs[i], dim, rs[i + 1]))\n",
    "            for i, (dim, key) in enumerate(zip(dims, keys))\n",
    "        ]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    # TT 的核心数据：n 个 core，每个 core 是 (r_left, dim, r_right)\n",
    "    cores: List[jnp.ndarray]\n",
    "\n",
    "    @property\n",
    "    def n_dims(self):\n",
    "        \"\"\"返回张量的维数 n（也就是 core 的个数）。\"\"\"\n",
    "        return len(self.cores)\n",
    "\n",
    "    @property\n",
    "    def full_tensor(self) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        把 TT 还原成“完整的高维张量”。\n",
    "\n",
    "        实现方式：\n",
    "          从第一个 core 开始，依次与后续 core 做 einsum 收缩 TT rank 维度。\n",
    "          每次收缩掉上一个结果的右 rank，与下一个 core 的左 rank 对齐。\n",
    "\n",
    "        结果形状：\n",
    "          (dim1, dim2, ..., dim_n)\n",
    "        \"\"\"\n",
    "        res = self.cores[0]  # (1, dim1, r2)\n",
    "        for core in self.cores[1:]:\n",
    "            # res:  (..., r)   core: (r, i, R)\n",
    "            # -> (..., i, R)  把 r 收缩掉，拼接出新的物理维度 i\n",
    "            res = jnp.einsum('...r,riR->...iR', res, core)\n",
    "\n",
    "        # TT 两端 rank 都是 1，所以第一维和最后一维可以 squeeze 掉\n",
    "        return jnp.squeeze(res, (0, -1))\n",
    "\n",
    "    def reverse(self) -> TT:\n",
    "        \"\"\"\n",
    "        把 TT 的维度顺序反过来（核心顺序翻转）。\n",
    "\n",
    "        注意：\n",
    "          翻转 core 的顺序后，每个 core 的左右 rank 方向也反了，\n",
    "          所以需要对 core 做 transpose_core 来交换左右 rank 轴。\n",
    "        \"\"\"\n",
    "        return TT([transpose_core(core) for core in self.cores[::-1]])\n",
    "\n",
    "    def astype(self, dtype: jnp.dtype) -> TT:\n",
    "        \"\"\"把 TT 的所有 core 转成指定 dtype（例如 float32 / float64）。\"\"\"\n",
    "        return TT([core.astype(dtype) for core in self.cores])\n",
    "\n",
    "    def __sub__(self, other: TT):\n",
    "        \"\"\"定义 TT 的减法：self - other。\"\"\"\n",
    "        return subtract(self, other)\n",
    "\n",
    "    def as_ttns(self) -> TTNS:\n",
    "        n_dims = self.n_dims\n",
    "        neighbors = [[] for _ in range(n_dims)]\n",
    "        for idx in range(n_dims):\n",
    "            if idx > 0:\n",
    "                neighbors[idx].append(idx - 1)\n",
    "            if idx < n_dims - 1:\n",
    "                neighbors[idx].append(idx + 1)\n",
    "        return TTNS(self.cores, neighbors, root=0)\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TTOperator:\n",
    "    \"\"\"\n",
    "    TT 格式的线性算子（你可以理解成高维矩阵/算子）。\n",
    "\n",
    "    如果普通矩阵是 2D：A[out, in]\n",
    "    那高维算子可以看成：A[i1..in, j1..jn]（输入 n 维 -> 输出 n 维）\n",
    "\n",
    "    TT Operator 用 n 个 4D core 表示，每个 core 形状：\n",
    "      (r_left, dim_from, dim_to, r_right)\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def generate_random(\n",
    "        cls, key: jnp.ndarray, dims_from: Sequence[int], dims_to: Sequence[int], rs: Sequence[int]\n",
    "    ) -> TTOperator:\n",
    "        \"\"\"\n",
    "        随机生成一个 TT Operator（每个 core 元素 ~ N(0,1)）。\n",
    "\n",
    "        参数：\n",
    "          key:       JAX random key\n",
    "          dims_from: 输入每维大小 [din1, din2, ..., din_n]\n",
    "          dims_to:   输出每维大小 [dout1, dout2, ..., dout_n]\n",
    "          rs:        内部 ranks（长度 n-1）\n",
    "\n",
    "        返回：\n",
    "          TTOperator 对象\n",
    "        \"\"\"\n",
    "        n_dims = len(dims_from)\n",
    "\n",
    "        # 基本一致性检查\n",
    "        assert len(dims_from) == n_dims\n",
    "        assert len(dims_to) == n_dims\n",
    "        assert len(rs) + 1 == n_dims\n",
    "\n",
    "        rs = [1] + list(rs) + [1]\n",
    "        keys = jax.random.split(key, n_dims)\n",
    "\n",
    "        # 每个 core 是 4D：(r_left, dim_from, dim_to, r_right)\n",
    "        cores = [\n",
    "            jax.random.normal(key, (rs[i], dim_from, dim_to, rs[i + 1]))\n",
    "            for i, (dim_from, dim_to, key) in enumerate(zip(dims_from, dims_to, keys))\n",
    "        ]\n",
    "\n",
    "        return cls(cores)\n",
    "\n",
    "    cores: List[jnp.ndarray]\n",
    "\n",
    "    @property\n",
    "    def full_operator(self) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        把 TT Operator 还原成完整算子（一个巨大的高维张量/矩阵）。\n",
    "\n",
    "        逐 core einsum：\n",
    "          res:  (..., r)\n",
    "          core: (r, i, j, R)\n",
    "          -> (..., i, j, R)\n",
    "\n",
    "        最终 squeeze 掉两端 rank=1。\n",
    "        结果形状：\n",
    "          (din1, dout1, din2, dout2, ..., din_n, dout_n)\n",
    "        （具体排列取决于 einsum 的写法，这里是按每个维度生成一对 (i,j)）\n",
    "        \"\"\"\n",
    "        res = self.cores[0]\n",
    "        for core in self.cores[1:]:\n",
    "            res = jnp.einsum('...r,rijR->...ijR', res, core) # Einstein求和约定\n",
    "        return jnp.squeeze(res, (0, -1)) \n",
    "\n",
    "    def reverse(self):\n",
    "        \"\"\"\n",
    "        反转 operator 的 core 顺序。\n",
    "\n",
    "        这里作者留了句注释：\n",
    "          \"idk, what should I do with axes 1 and 2.\"\n",
    "        意思是：输入/输出物理轴 (dim_from, dim_to) 是否要交换、怎么交换，\n",
    "        其实取决于你希望 reverse 后代表什么数学对象（是反转维度顺序？还是转置算子？）。\n",
    "\n",
    "        当前实现：\n",
    "          - core 顺序翻转\n",
    "          - 把 rank 轴对调：把 axis 0 和 axis 3 互换\n",
    "          - axis 1/2（输入/输出物理轴）保持不变\n",
    "        \"\"\"\n",
    "        return TTOperator(\n",
    "            [jnp.moveaxis(core, (0, 1, 2, 3), (3, 1, 2, 0)) for core in self.cores[::-1]]\n",
    "        )\n",
    "    def astype(self, dtype: jnp.dtype) -> \"TTOperator\":\n",
    "        return TTOperator([core.astype(dtype) for core in self.cores])\n",
    "\n",
    "\n",
    "def transpose_core(core: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    对 TT core 做“左右 rank 交换”。\n",
    "\n",
    "    输入 core 形状：(r_left, dim, r_right)\n",
    "    输出形状：(r_right, dim, r_left)\n",
    "\n",
    "    用途：\n",
    "      TT.reverse() 翻转 core 顺序时，需要把连接方向也翻过来。\n",
    "    \"\"\"\n",
    "    return jnp.moveaxis(core, (0, 1, 2), (2, 1, 0))\n",
    "\n",
    "\n",
    "def subtract(lhs: TT, rhs: TT) -> TT:\n",
    "    \"\"\"\n",
    "    计算两个 TT 的差：lhs - rhs，并返回一个新的 TT。\n",
    "\n",
    "    关键点（非常重要）：\n",
    "    - 一般 TT 直接相减后，结果的 TT rank 会变大\n",
    "    - 这里用的是经典的“block-diagonal 拼接”构造法：\n",
    "        - 第一个 core：在右 rank 方向拼接 [lhs, -rhs]\n",
    "        - 中间 core：做 2x2 的块对角拼接（lhs 在左上，rhs 在右下）\n",
    "        - 最后一个 core：在左 rank 方向拼接 [lhs; rhs]\n",
    "\n",
    "    这样保证：\n",
    "      TT(full) = TT(lhs) - TT(rhs)\n",
    "\n",
    "    代价：\n",
    "      ranks 会变成原来的大约“相加”（更准确：中间 rank 变成 r1+r2）。\n",
    "    \"\"\"\n",
    "    assert lhs.n_dims == rhs.n_dims\n",
    "\n",
    "    # 只有 1 个维度时，TT 就是一个 (1, dim, 1) 的 core，直接相减即可\n",
    "    if lhs.n_dims == 1:\n",
    "        return TT([lhs.cores[0] - rhs.cores[0]])\n",
    "\n",
    "    # 第一个 core：沿着右 rank 维（axis=-1）拼接\n",
    "    # lhs: (1, d1, r1)  rhs: (1, d1, r1')\n",
    "    # -> (1, d1, r1+r1')\n",
    "    first = jnp.concatenate([lhs.cores[0], -rhs.cores[0]], axis=-1)\n",
    "\n",
    "    # 最后一个 core：沿着左 rank 维（axis=0）拼接\n",
    "    # lhs: (r_{n-1}, dn, 1)  rhs: (r'_{n-1}, dn, 1)\n",
    "    # -> (r_{n-1}+r'_{n-1}, dn, 1)\n",
    "    last = jnp.concatenate([lhs.cores[-1], rhs.cores[-1]], axis=0)\n",
    "\n",
    "    # 中间 core：做块对角拼接（2x2 block）\n",
    "    # 对每个位置 k：\n",
    "    #   [ c1   0 ]\n",
    "    #   [ 0   c2 ]\n",
    "    inner = [\n",
    "        jnp.concatenate(\n",
    "            [\n",
    "                # 上半块： [c1, 0]\n",
    "                jnp.concatenate([c1, jnp.zeros((c1.shape[0], c1.shape[1], c2.shape[2]))], axis=-1),\n",
    "                # 下半块： [0, c2]\n",
    "                jnp.concatenate([jnp.zeros((c2.shape[0], c2.shape[1], c1.shape[2])), c2], axis=-1),\n",
    "            ],\n",
    "            axis=0,  # 沿左 rank 方向拼接成上下两块\n",
    "        )\n",
    "        for c1, c2 in zip(lhs.cores[1:-1], rhs.cores[1:-1])\n",
    "    ]\n",
    "\n",
    "    return TT([first] + inner + [last])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810ef9e",
   "metadata": {},
   "source": [
    "+ 上面加载ttns的基本运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8208d1a",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def assert_allclose(a, b, rtol=1e-5, atol=1e-6, name=\"\"):\n",
    "    ok = jnp.allclose(a, b, rtol=rtol, atol=atol)\n",
    "    if not bool(ok):\n",
    "        max_err = jnp.max(jnp.abs(a - b))\n",
    "        raise AssertionError(f\"[{name}] not close, max|err|={float(max_err)}\")\n",
    "\n",
    "def tt_eval_by_definition(tt, idxs):\n",
    "    # idxs: tuple(i1,...,id)\n",
    "    res = tt.cores[0][:, idxs[0], :]          # shape (1, r1)\n",
    "    for k in range(1, tt.n_dims):\n",
    "        res = res @ tt.cores[k][:, idxs[k], :]  # (1, rk)\n",
    "    return res[0, 0]\n",
    "\n",
    "def test_tt_full_tensor(tt):\n",
    "    dense = tt.full_tensor\n",
    "    d = tt.n_dims\n",
    "    dims = dense.shape\n",
    "    # 全枚举（小规模才跑）\n",
    "    for flat in range(int(jnp.prod(jnp.array(dims)))):\n",
    "        idxs = jnp.unravel_index(flat, dims)\n",
    "        val_def = tt_eval_by_definition(tt, tuple(int(x) for x in idxs))\n",
    "        assert_allclose(val_def, dense[idxs], name=\"TT.full_tensor vs definition\")\n",
    "\n",
    "def op_eval_by_definition(op, ins, outs):\n",
    "    # ins: (i1,...,id), outs: (j1,...,jd)\n",
    "    # core shape: (rL, dim_from, dim_to, rR) => (rL, i, j, rR)\n",
    "    res = op.cores[0][:, ins[0], outs[0], :]  # (1, r1)\n",
    "    for k in range(1, len(op.cores)):\n",
    "        res = res @ op.cores[k][:, ins[k], outs[k], :]\n",
    "    return res[0, 0]\n",
    "\n",
    "def test_op_full_operator(op):\n",
    "    full = op.full_operator\n",
    "    d = len(op.cores)\n",
    "    din = [op.cores[k].shape[1] for k in range(d)]\n",
    "    dout = [op.cores[k].shape[2] for k in range(d)]\n",
    "\n",
    "    # 全枚举（小规模才跑）\n",
    "    for flat_in in range(int(jnp.prod(jnp.array(din)))):\n",
    "        ins = jnp.unravel_index(flat_in, din)\n",
    "        ins = tuple(int(x) for x in ins)\n",
    "        for flat_out in range(int(jnp.prod(jnp.array(dout)))):\n",
    "            outs = jnp.unravel_index(flat_out, dout)\n",
    "            outs = tuple(int(x) for x in outs)\n",
    "\n",
    "            # full 的索引是 (i1,j1,i2,j2,...)\n",
    "            idx = []\n",
    "            for k in range(d):\n",
    "                idx += [ins[k], outs[k]]\n",
    "            idx = tuple(idx)\n",
    "\n",
    "            val_def = op_eval_by_definition(op, ins, outs)\n",
    "            assert_allclose(val_def, full[idx], name=\"TTOperator.full_operator vs definition\")\n",
    "\n",
    "\n",
    "# 假设你已经有 normalized_dot_operator, TTOpt, TTOperatorOpt\n",
    "def test_dot_operator(tt, op):\n",
    "    # dense X\n",
    "    X = tt.full_tensor                         # shape (m1,...,md)\n",
    "    m = X.shape\n",
    "    d = len(m)\n",
    "\n",
    "    # dense A (交错轴: i1,j1,i2,j2,...)\n",
    "    A = op.full_operator                       # shape (m1,n1,m2,n2,...)\n",
    "    n = tuple(op.cores[k].shape[2] for k in range(d))\n",
    "\n",
    "    # 把 A 转成矩阵 [J, I]\n",
    "    # 先把交错轴分组为 (i1,i2,...,id, j1,j2,...,jd) 或 (j..., i...) 都行，统一即可\n",
    "    # 这里把 A 重新排列到 (j1,...,jd, i1,...,id)\n",
    "    perm = []\n",
    "    # 交错轴位置：i_k 在 2k，j_k 在 2k+1（0-based）\n",
    "    for k in range(d):\n",
    "        perm.append(2*k+1)   # j_k\n",
    "    for k in range(d):\n",
    "        perm.append(2*k)     # i_k\n",
    "    A_mat = jnp.transpose(A, perm)             # shape (n1..nd, m1..md)\n",
    "    A_mat = A_mat.reshape(int(jnp.prod(jnp.array(n))), int(jnp.prod(jnp.array(m))))\n",
    "\n",
    "    X_vec = X.reshape(-1)\n",
    "    Y_true = (A_mat @ X_vec).reshape(n)\n",
    "\n",
    "    # TT 侧计算\n",
    "    tt_opt = TTOpt.from_tt(tt)\n",
    "    op_opt = TTOperatorOpt.from_tt_operator(op)\n",
    "    y_opt = normalized_dot_operator(tt_opt, op_opt)\n",
    "    Y = y_opt.to_nonopt_tt().full_tensor       # dense output\n",
    "\n",
    "    assert_allclose(Y, Y_true, name=\"normalized_dot_operator correctness\")\n",
    "\n",
    "\n",
    "def recover_scalar(nv):\n",
    "    # nv.value 可能是 (1,1)；统一 squeeze\n",
    "    return jnp.exp(nv.log_norm) * jnp.squeeze(nv.value)\n",
    "\n",
    "def test_normalized_inner_product(tt1, tt2):\n",
    "    X = tt1.full_tensor.reshape(-1)\n",
    "    Y = tt2.full_tensor.reshape(-1)\n",
    "    true_ip = jnp.vdot(X, Y)  # 实数时就是 sum(X*Y)\n",
    "\n",
    "    nv = normalized_inner_product(TTOpt.from_tt(tt1), TTOpt.from_tt(tt2))\n",
    "    ip = recover_scalar(nv)\n",
    "\n",
    "    assert_allclose(ip, true_ip, name=\"normalized_inner_product correctness\")\n",
    "\n",
    "def test_subtract(lhs, rhs):\n",
    "    out = (lhs - rhs).full_tensor\n",
    "    true = lhs.full_tensor - rhs.full_tensor\n",
    "    assert_allclose(out, true, name=\"TT subtract correctness\")\n",
    "\n",
    "def test_reverse(tt):\n",
    "    X = tt.full_tensor\n",
    "    Y = tt.reverse().full_tensor\n",
    "    true = jnp.transpose(X, axes=list(range(tt.n_dims))[::-1])\n",
    "    assert_allclose(Y, true, name=\"TT reverse correctness\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b2736d4",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TTOperator' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     test_reverse(tt1)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mALL TESTS PASSED\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mrun_all_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mrun_all_tests\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m tt2 = TT.generate_random(jax.random.PRNGKey(\u001b[32m1\u001b[39m), dims, rs).astype(jnp.float64)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 生成算子（这里用方阵：dims_from=dims_to）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m op = \u001b[43mTTOperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_random\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m(jnp.float64)\n\u001b[32m     16\u001b[39m test_tt_full_tensor(tt1)\n\u001b[32m     17\u001b[39m test_tt_full_tensor(tt2)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TTOperator' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "def run_all_tests():\n",
    "    key = jax.random.PRNGKey(0)\n",
    "\n",
    "    # 小规模\n",
    "    dims = [3, 2, 4, 3]   # d=4\n",
    "    rs   = [2, 3, 2]      # 长度 d-1\n",
    "\n",
    "    # 生成 TT\n",
    "    from ttde.tt.tensors import TT, TTOperator  # 你项目里真实 import\n",
    "    tt1 = TT.generate_random(key, dims, rs).astype(jnp.float64)\n",
    "    tt2 = TT.generate_random(jax.random.PRNGKey(1), dims, rs).astype(jnp.float64)\n",
    "\n",
    "    # 生成算子（这里用方阵：dims_from=dims_to）\n",
    "    op = TTOperator.generate_random(jax.random.PRNGKey(2), dims, dims, rs).astype(jnp.float64)\n",
    "\n",
    "    test_tt_full_tensor(tt1)\n",
    "    test_tt_full_tensor(tt2)\n",
    "\n",
    "    test_op_full_operator(op)\n",
    "\n",
    "    test_normalized_inner_product(tt1, tt2)\n",
    "\n",
    "    test_dot_operator(tt1, op)\n",
    "\n",
    "    test_subtract(tt1, tt2)\n",
    "    test_reverse(tt1)\n",
    "\n",
    "    print(\"ALL TESTS PASSED\")\n",
    "run_all_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
